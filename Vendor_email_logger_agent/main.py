import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import asyncio
import logging
from datetime import datetime, timedelta
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from Vendor_email_logger_agent.config import settings, AgentSettings
import openai
from google.auth.transport.requests import Request
from dateutil import parser

from Vendor_email_logger_agent.src.gmail.gmail_watcher import GmailWatcher
from Vendor_email_logger_agent.src.gmail.email_collector import EmailCollector
from Vendor_email_logger_agent.src.utils.text_processor import TextProcessor
from Vendor_email_logger_agent.src.processors.email_processor import EmailProcessor
from Vendor_email_logger_agent.src.processors.attachment_processor import AttachmentProcessor
from Vendor_email_logger_agent.src.services.mcp_service import MCPService
from Vendor_email_logger_agent.src.services.supabase_service import SupabaseService
from Vendor_email_logger_agent.src.gmail.message_filter import VendorEmailManager, is_vendor_email
from external_communication.utils.email_utils import get_gmail_service
from Vendor_email_logger_agent.config import supabase

# Load settings
settings = AgentSettings()

# Initialize OpenAI API key
openai.api_key = settings.OPENAI_API_KEY

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
log_dir = os.path.join(os.path.dirname(__file__), 'logs')
os.makedirs(log_dir, exist_ok=True)

# ë¡œê¹… ì„¤ì •
log_file = os.path.join(log_dir, f'email_logger_{datetime.now().strftime("%Y%m%d")}.log')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

CREDENTIALS_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'credentials.json')

def authenticate_gmail():
    """Gmail API ì¸ì¦"""
    creds = None
    credentials_path = CREDENTIALS_PATH
    token_path = os.path.join(os.path.dirname(__file__), 'credentials', 'token.json')
    
    if os.path.exists(token_path):
        creds = Credentials.from_authorized_user_file(token_path, settings.GMAIL_SCOPES)
    else:
        if not os.path.exists(credentials_path):
            raise FileNotFoundError(f"credentials.json file not found at {credentials_path}")
            
        flow = InstalledAppFlow.from_client_secrets_file(
            credentials_path, 
            settings.GMAIL_SCOPES
        )
        creds = flow.run_local_server(port=8002)
        with open(token_path, 'w') as token:
            token.write(creds.to_json())
    return build('gmail', 'v1', credentials=creds)

async def process_email(service, msg, email_processor: EmailProcessor, mcp_service: MCPService, vendor_manager: VendorEmailManager):
    """ì´ë©”ì¼ ì²˜ë¦¬"""
    try:
        msg_id = msg['id']
        message = service.users().messages().get(
            userId='me',
            id=msg_id,
            format='metadata',
            metadataHeaders=['Subject', 'From', 'Date', 'To']
        ).execute()
        headers = message.get("payload", {}).get("headers", [])
        subject = next((h['value'] for h in headers if h['name'] == 'Subject'), '')
        from_email = next((h['value'] for h in headers if h['name'] == 'From'), '')
        to_email = next((h['value'] for h in headers if h['name'] == 'To'), '')
        sent_at = next((h['value'] for h in headers if h['name'] == 'Date'), '')
        direction = "outbound" if 'SENT' in message.get('labelIds', []) else "inbound"
        logger.info(f"[DBì €ì¥ì „] direction: {direction}, msg_id: {msg_id}, from: {from_email}, to: {to_email}")
        content = email_processor.get_message_content(msg_id)
        parsed_message = {
            "thread_id": message.get("threadId"),
            "message_id": msg_id,
            "subject": subject,
            "from": from_email,
            "to": to_email,
            "sent_at": sent_at,
            "body_text": content["body_text"],
            "direction": direction,
            "attachments": content["attachments"]
        }
        await email_processor.save_email_log(parsed_message)
        await mcp_service.send_message(parsed_message)
    except Exception as e:
        logger.error(f"Error processing email {msg['id']}: {str(e)}")
        raise

async def collect_historical_emails(service, email_processor: EmailProcessor, mcp_service: MCPService, vendor_manager: VendorEmailManager, months_back=1):
    """ê³¼ê±° ì´ë©”ì¼ ìˆ˜ì§‘"""
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ë³´ë‚¸ ì´ë©”ì¼ê³¼ ë°›ì€ ì´ë©”ì¼ ëª¨ë‘ í¬í•¨)
        query = f'after:{(datetime.utcnow() - timedelta(days=30*months_back)).strftime("%Y/%m/%d")}'
        logger.info(f"Searching emails with query: {query}")
        
        # ë°›ì€ ì´ë©”ì¼ ê²€ìƒ‰
        results = service.users().messages().list(
            userId='me',
            labelIds=['INBOX'],
            q=query,
            maxResults=700
        ).execute()
        
        messages = results.get('messages', [])
        logger.info(f"INBOXì—ì„œ ê°€ì ¸ì˜¨ ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
        for msg in messages:
            logger.info(f"INBOX ë©”ì‹œì§€ ID: {msg['id']}")
        
        # ë³´ë‚¸ ì´ë©”ì¼ ê²€ìƒ‰
        sent_results = service.users().messages().list(
            userId='me',
            labelIds=['SENT'],
            q=query,
            maxResults=700
        ).execute()
        
        sent_messages = sent_results.get('messages', [])
        # logger.info(f"Found {len(sent_messages)} sent messages")
        
        # ë°›ì€ ë©”ì¼ + ë³´ë‚¸ ë©”ì¼
        all_messages = messages + sent_messages
        
        # ë©”ì‹œì§€ë¥¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        def clean_date_str(date_str):
            import re
            # 1. +0000 (UTC) â†’ +0000
            date_str = re.sub(r"([+-][0-9]{4}) ?\([^)]+\)", r"\1", date_str)
            # 2. +0000 +0000 â†’ ë§ˆì§€ë§‰ë§Œ ë‚¨ê¸°ê¸°
            date_str = re.sub(r"([+-][0-9]{4}) ?([+-][0-9]{4})", r"\2", date_str)
            # 3. ë‚¨ì€ ê´„í˜¸ ë° ì• ê³µë°± ì œê±°
            date_str = re.sub(r" ?\([^)]+\)", "", date_str)
            # 4. ì—¬ëŸ¬ ê³µë°± ì •ë¦¬
            date_str = re.sub(r" +", " ", date_str).strip()
            logger.debug(f"[clean_date_str] after clean: '{date_str}'")
            return date_str

        def get_msg_date(msg):
            try:
                message = service.users().messages().get(
                    userId='me',
                    id=msg['id'],
                    format='metadata',
                    metadataHeaders=['Date']
                ).execute()
                headers = message.get("payload", {}).get("headers", [])
                date_str = next((h['value'] for h in headers if h['name'] == 'Date'), '')
                date_str = clean_date_str(date_str)
                logger.debug(f"[get_msg_date] final date_str: '{date_str}'")
                try:
                    # dateutil.parserë¡œ ë‚ ì§œ íŒŒì‹±
                    dt = parser.parse(date_str)
                    dt = dt.astimezone(__import__('datetime').timezone.utc)
                    return dt
                except Exception as e:
                    logger.warning(f"Failed to parse date for msg {msg['id']}: {e}")
                    from datetime import timezone
                    return datetime.max.replace(tzinfo=timezone.utc)
            except Exception as e:
                logger.warning(f"Failed to parse date for msg {msg['id']}: {e}")
                from datetime import timezone
                return datetime.max.replace(tzinfo=timezone.utc)

        all_messages.sort(key=get_msg_date)

        for msg in all_messages:
            try:
                message = service.users().messages().get(
                    userId='me',
                    id=msg['id'],
                    format='metadata',
                    metadataHeaders=['Subject', 'From', 'Date', 'To']
                ).execute()
                # í•„í„° ì „ ì •ë³´ ì¶œë ¥
                headers = message.get("payload", {}).get("headers", [])
                subject = next((h['value'] for h in headers if h['name'] == 'Subject'), '')
                from_email = next((h['value'] for h in headers if h['name'] == 'From'), '')
                to_email = next((h['value'] for h in headers if h['name'] == 'To'), '')
                logger.info(f"Processing message: {msg['id']}, from: {from_email}, to: {to_email}")
                logger.info(f"ë²¤ë” ì´ë©”ì¼ ë¦¬ìŠ¤íŠ¸: {vendor_manager.vendor_emails}")
                result = is_vendor_email(message, vendor_manager)
                logger.info(f"is_vendor_email ê²°ê³¼: {result}")
                if result:
                    await process_email(service, msg, email_processor, mcp_service, vendor_manager)
            except Exception as e:
                logger.error(f"Error processing message {msg['id']}: {e}")
                continue
            
    except Exception as e:
        logger.error(f"Error collecting historical emails: {e}")

async def watch_new_vendor_emails(service, email_processor, mcp_service, vendor_manager):
    """10ë¶„ë§ˆë‹¤ purchase_orders í…Œì´ë¸”ì—ì„œ vendor_emailì„ ì¡°íšŒí•´ ìƒˆë¡œìš´ ì´ë©”ì¼ì´ ìˆìœ¼ë©´ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ íŠ¸ë¦¬ê±°"""
    supabase_service = SupabaseService()
    while True:
        try:
            # DBì—ì„œ vendor_email ëª©ë¡ ì¬ì¡°íšŒ
            result = supabase_service.client.from_("purchase_orders").select("vendor_email").not_.is_("vendor_email", "null").execute()
            db_emails = set(row["vendor_email"].strip().lower() for row in result.data if row.get("vendor_email"))
            # ìƒˆë¡œ ë°œê²¬ëœ ì´ë©”ì¼
            new_emails = db_emails - vendor_manager.vendor_emails
            if new_emails:
                for email in new_emails:
                    print(f"[NEW VENDOR EMAIL] {email} ë°œê²¬! íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹œì‘...")
                    # ë²¤ë” ì´ë©”ì¼ setì— ì¶”ê°€
                    vendor_manager.vendor_emails.add(email)
                    # í•´ë‹¹ ì´ë©”ì¼ì— ëŒ€í•œ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ íŠ¸ë¦¬ê±°
                    # (collect_historical_emailsëŠ” ì „ì²´ë¥¼ ë„ëŠ” êµ¬ì¡°ë¼ë©´, ì´ë©”ì¼ í•„í„°ë§ ì¶”ê°€ í•„ìš”)
                    # ì•„ë˜ëŠ” ì˜ˆì‹œ: í•´ë‹¹ ì´ë©”ì¼ë§Œ ëŒ€ìƒìœ¼ë¡œ ìˆ˜ì§‘
                    await collect_historical_emails_for_vendor(service, email_processor, mcp_service, vendor_manager, email)
            await asyncio.sleep(600)  # 10ë¶„ë§ˆë‹¤ ë°˜ë³µ
        except Exception as e:
            print(f"[watch_new_vendor_emails] Error: {e}")
            await asyncio.sleep(60)

async def collect_historical_emails_for_vendor(service, email_processor, mcp_service, vendor_manager, vendor_email):
    """íŠ¹ì • ë²¤ë” ì´ë©”ì¼ì— ëŒ€í•œ ê³¼ê±° ì´ë©”ì¼ë§Œ ìˆ˜ì§‘"""
    # Gmail APIì—ì„œ í•´ë‹¹ ë²¤ë” ì´ë©”ì¼ê³¼ ê´€ë ¨ëœ ê³¼ê±° ì´ë©”ì¼ë§Œ ê²€ìƒ‰
    # (ì˜ˆì‹œ: from:vendor_email OR to:vendor_email)
    query = f'from:{vendor_email} OR to:{vendor_email}'
    print(f"[HISTORY] {vendor_email} ê³¼ê±° ì´ë©”ì¼ ìˆ˜ì§‘ ì¿¼ë¦¬: {query}")
    try:
        results = service.users().messages().list(
            userId='me',
            q=query,
            maxResults=300
        ).execute()
        messages = results.get('messages', [])
        for msg in messages:
            try:
                message = service.users().messages().get(
                    userId='me',
                    id=msg['id'],
                    format='metadata',
                    metadataHeaders=['Subject', 'From', 'Date', 'To']
                ).execute()
                if is_vendor_email(message, vendor_manager):
                    await process_email(service, msg, email_processor, mcp_service, vendor_manager)
            except Exception as e:
                print(f"[HISTORY] Error processing message for {vendor_email}: {e}")
    except Exception as e:
        print(f"[HISTORY] Error collecting historical emails for {vendor_email}: {e}")

async def fetch_and_save_user_inbox(user_row, email_processor):
    try:
        service = get_gmail_service(user_row)
        results = service.users().messages().list(userId='me', labelIds=['INBOX'], maxResults=100).execute()
        messages = results.get('messages', [])
        for msg in messages:
            try:
                message = service.users().messages().get(userId='me', id=msg['id'], format='metadata', metadataHeaders=['Subject', 'From', 'Date', 'To']).execute()
                headers = message.get("payload", {}).get("headers", [])
                subject = next((h['value'] for h in headers if h['name'] == 'Subject'), '')
                from_email = next((h['value'] for h in headers if h['name'] == 'From'), '')
                to_email = next((h['value'] for h in headers if h['name'] == 'To'), '')
                sent_at = next((h['value'] for h in headers if h['name'] == 'Date'), '')
                direction = "inbound"
                content = email_processor.get_message_content(msg['id'])
                parsed_message = {
                    "thread_id": message.get("threadId"),
                    "message_id": msg['id'],
                    "subject": subject,
                    "from": from_email,
                    "to": to_email,
                    "sent_at": sent_at,
                    "body_text": content["body_text"],
                    "direction": direction,
                    "attachments": content["attachments"]
                }
                await email_processor.save_email_log(parsed_message)
            except Exception as e:
                logger.error(f"[{user_row['email']}] Error processing message {msg['id']}: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"[{user_row['email']}] Gmail token invalid or expired: {e}", exc_info=True)

async def run_for_user(user_row):
    try:
        # 1. Gmail ì¸ì¦
        service = get_gmail_service(user_row)
        if service is None:
            logger.error(f"[{user_row['email']}] Gmail service ìƒì„± ì‹¤íŒ¨ (None ë°˜í™˜). í† í°/ì¸ì¦ ì •ë³´ í™•ì¸ í•„ìš”.")
            return

        # ğŸ”‘ credentials ê°±ì‹  ë¡œì§ (ê¸°ì¡´ ê·¸ëŒ€ë¡œ ìœ ì§€)
        credentials = service._http.credentials
        if not credentials.valid and credentials.expired and credentials.refresh_token:
            try:
                credentials.refresh(Request())
                logger.info(f"[{user_row['email']}] âœ… Token refreshed successfully")

                supabase.table("users").update({
                    "email_access_token": credentials.token,
                    "email_token_expiry": credentials.expiry.isoformat(),
                    "email_token_json": credentials.to_json()
                }).eq("id", user_row["id"]).execute()
                logger.info(f"[{user_row['email']}] âœ… Token updated in Supabase")
            except Exception as e:
                logger.error(f"[{user_row['email']}] âŒ Failed to refresh token: {e}")
                return

        # âœ… 2. ì´ ìœ ì €ì˜ vendor_emailë§Œ ê°€ì ¸ì˜¤ê¸°
        po_result = supabase.table("purchase_orders").select("vendor_email") \
            .eq("user_id", user_row["id"]) \
            .not_.is_("vendor_email", "null") \
            .execute()

        vendor_emails = set(
            row["vendor_email"].lower().strip()
            for row in po_result.data
            if row.get("vendor_email")
        )

        logger.info(f"[{user_row['email']}] ğŸ” Loaded {len(vendor_emails)} vendor emails")

        # âœ… VendorEmailManagerì— ë„˜ê¹€
        vendor_manager = VendorEmailManager(vendor_emails=vendor_emails)

        # 3. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        text_processor = TextProcessor()
        mcp_service = MCPService()
        supabase_service = SupabaseService()
        email_processor = EmailProcessor(service, text_processor, supabase_client=supabase_service)
        watcher = GmailWatcher(service, vendor_manager)

        logger.info(f"[{user_row['email']}] GmailWatcher initialized")

        # 4. ì´ë©”ì¼ ìˆ˜ì§‘ ë° ì‹¤ì‹œê°„ ê°ì‹œ ë³‘ë ¬ ì‹¤í–‰
        await asyncio.gather(
            collect_historical_emails(service, email_processor, mcp_service, vendor_manager, months_back=1),
            watch_new_vendor_emails(service, email_processor, mcp_service, vendor_manager),
            asyncio.to_thread(watcher.watch, lambda email: asyncio.create_task(
                process_email(service, email, email_processor, mcp_service, vendor_manager)))
        )

    except Exception as e:
        logger.error(f"[{user_row.get('email', 'unknown')}] Error in run_for_user: {e}")

async def main():
    # Supabaseì—ì„œ email_access_tokenì´ ìˆëŠ” ëª¨ë“  ìœ ì € ì¡°íšŒ
    users = supabase.table("users").select("*").not_.is_("email_access_token", "null").execute().data
    tasks = [run_for_user(user) for user in users]
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
